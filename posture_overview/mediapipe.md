# MediaPipe 자세 추정 기능 도입 계획

## 1. 목표

기존의 동영상 업로드 기반 자세 분석 기능(`PostureAnalysisPage.tsx`)을 개선하기에 앞서, **별도의 실험용 페이지**에서 **MediaPipe를 이용한 실시간 자세 추정 기능**의 기술적 타당성을 검증하고 프로토타입을 개발한다. 이 검증이 완료되면, 핵심 로직을 기존 기능에 통합하거나 고도화한다.

## 2. 개발 단계

### 1단계: 실험용 페이지 및 라우팅 설정

1.  **새 페이지 컴포넌트 생성:**
    *   `src/pages/tools/` 디렉토리에 `RealtimePostureGuide.tsx` 라는 이름의 새 파일을 생성한다.
    *   이 파일은 실시간 카메라 피드와 MediaPipe 분석 결과를 렌더링하는 역할을 담당한다.

2.  **라우팅 추가:**
    *   메인 라우터 파일(예: `App.tsx` 또는 `src/App.tsx` 내의 라우팅 설정)에 새로운 경로를 추가한다.
    *   `/app/tools/realtime-posture-guide` 경로로 접속했을 때 `RealtimePostureGuide.tsx` 컴포넌트가 렌더링되도록 설정한다.
    *   개발 중 쉽게 접근할 수 있도록, 임시로 네비게이션 메뉴에 링크를 추가할 수 있다.

### 2단계: 실시간 카메라 및 MediaPipe 연동

1.  **필요 라이브러리 설치:**
    *   MediaPipe의 Pose 솔루션을 사용하기 위한 패키지를 설치한다.
        ```bash
        npm install @mediapipe/pose @mediapipe/camera_utils @mediapipe/drawing_utils
        ```

2.  **카메라 연동 구현 (`RealtimePostureGuide.tsx`):**
    *   `useRef`를 사용하여 `<video>` 엘리먼트와 `<canvas>` 엘리먼트에 대한 참조를 생성한다.
    *   `useEffect` 훅을 사용하여 컴포넌트가 마운트될 때 사용자의 카메라에 접근 (`navigator.mediaDevices.getUserMedia`) 하고, 비디오 스트림을 `<video>` 엘리먼트에 연결한다.

3.  **MediaPipe Pose 초기화 및 실행:**
    *   `@mediapipe/pose`에서 `Pose` 객체를 초기화한다. 모델 복잡도, 신뢰도 임계값 등 필요한 옵션을 설정한다.
    *   `@mediapipe/camera_utils`의 `Camera` 유틸리티를 사용하여 `<video>` 엘리먼트를 MediaPipe `Pose` 인스턴스에 연결한다.
    *   `pose.onResults()` 콜백 함수를 정의한다. 이 함수는 MediaPipe가 매 프레임 분석을 완료할 때마다 호출된다.

### 3단계: 분석 결과 시각화 및 피드백 구현

1.  **결과 시각화 (`onResults` 콜백 내부):**
    *   `@mediapipe/drawing_utils`를 사용하여 `<canvas>` 위에 분석 결과를 그린다.
    *   `drawConnectors` 함수로 감지된 관절들을 연결하여 스켈레톤을 그린다.
    *   `drawLandmarks` 함수로 감지된 주요 관절 포인트를 그린다.

2.  **실시간 피드백 로직 구현:**
    *   `onResults` 콜백에서 반환되는 `poseLandmarks` (주요 관절 좌표) 데이터를 분석한다.
    *   **화면 내 객체 크기/위치 판단:**
        *   강아지의 스켈레톤이 차지하는 영역을 계산하여 "더 가까이 오세요" 또는 "너무 가깝습니다" 와 같은 피드백을 화면에 텍스트로 표시한다.
        *   스켈레톤이 화면 중앙에 위치하도록 "왼쪽으로 이동하세요" 와 같은 가이드를 제공한다.
    *   **자세 판단 (예: 옆모습):**
        *   왼쪽 어깨/엉덩이와 오른쪽 어깨/엉덩이의 x좌표를 비교하여 강아지가 옆을 보고 있는지 대략적으로 판단하고, "옆모습을 보여주세요" 와 같은 안내를 표시한다.
    *   모든 조건이 충족되면 "자세가 좋습니다! 이대로 촬영을 진행하세요." 와 같은 긍정적 피드백을 표시한다.

## 3. 검증 및 향후 계획

*   **검증:** 위 3단계가 완료되면, `RealtimePostureGuide.tsx` 페이지가 스마트폰 및 데스크탑 브라우저에서 의도대로 작동하는지 확인한다. (성능, 정확도, 사용자 경험)
*   **향후 계획:**
    *   프로토타입이 성공적으로 검증되면, 이 페이지에서 구현된 핵심 로직(`MediaPipe` 연동, 결과 분석 및 피드백)을 기존 `PostureAnalysisPage.tsx`에 통합하는 작업을 계획한다.
    *   통합 시에는, 동영상 업로드 방식과 실시간 분석 방식을 사용자가 선택할 수 있도록 UI를 개선하거나, 업로드 전에 실시간 가이드를 먼저 보여주는 흐름으로 UX를 개선할 수 있다.

---

## 4. 실시간 자세 가이드 기능 개발 보고서

### 4.1. 최종 성과

초기 계획에 따라 기술 검증을 성공적으로 완료하였으며, 사용자가 직접 제어할 수 있는 안정적인 실시간 자세 가이드 프로토타입을 개발했습니다. 이 기능은 기존의 정적인 동영상 업로드 방식의 사용자 경험을 크게 개선할 수 있는 잠재력을 보여주었습니다.

*   **주요 성과:**
    *   **사용자 제어 기능:** '시작/중지' 버튼을 통해 사용자가 원할 때만 카메라와 AI 분석 기능을 활성화/비활성화할 수 있습니다.
    *   **실시간 스켈레톤 렌더링:** 웹캠 영상 위에 끊김 없이 자세 스켈레톤을 그려줍니다.
    *   **동적 피드백 시스템:** 감지된 자세의 화면 내 크기, 위치, 선명도에 따라 사용자에게 유용한 피드백을 텍스트로 제공합니다.
    *   **안정적인 아키텍처 확립:** 수많은 디버깅 끝에, React 환경에서 MediaPipe를 가장 안정적으로 구동할 수 있는 코드 구조와 핵심 원리를 확립했습니다.

### 4.2. 핵심 파일 분석: `src/pages/tools/RealtimePostureGuide.tsx`

이 파일은 실시간 자세 가이드 기능의 모든 로직을 담고 있는 단일 컴포넌트입니다.

*   **역할:** AI 모델 로딩, 카메라 제어, 실시간 예측, 결과 시각화 등 기능의 모든 측면을 담당합니다.

*   **주요 기능:**
    *   **AI 모델 로딩:** 컴포넌트 로딩 시 MediaPipe PoseLandmarker 모델을 비동기적으로 불러옵니다.
    *   **UI 컨트롤:** 사용자가 기능을 제어할 수 있는 '시작하기'와 '중지하기' 버튼을 제공합니다.
    *   **실시간 카메라 피드:** 웹캠 영상을 화면에 표시합니다.
    *   **스켈레톤 렌더링:** 감지된 자세의 랜드마크(관절)와 커넥터(뼈대)를 비디오 위에 실시간으로 오버레이하여 그립니다.
    *   **상황별 텍스트 피드백:** 감지된 자세의 유효성, 화면 내 크기 등을 분석하여 사용자에게 안내 메시지를 보여줍니다.

*   **작동 원리 (단계별):**
    1.  **초기화 (`useEffect`):** 컴포넌트가 처음 렌더링될 때, `useEffect` 훅이 실행됩니다. 이 훅은 MediaPipe의 `PoseLandmarker` 모델을 비동기적으로 생성하고, `poseLandmarkerRef`에 참조를 저장합니다. **안정성이 검증된 `0.10.3` 버전의 WASM 파일을 사용합니다.**
    2.  **사용자 시작 (`startWebcam`):** 사용자가 '시작하기' 버튼을 클릭하면 `startWebcam` 함수가 호출됩니다. 이 함수는 `navigator.mediaDevices.getUserMedia`를 통해 카메라 권한을 요청하고 비디오 스트림을 가져옵니다. 비디오 데이터가 로드되면(`loadeddata` 이벤트), `isCameraOn` 상태를 `true`로 설정합니다.
    3.  **루프 제어 (`useEffect` on `isCameraOn`):** `isCameraOn` 상태를 감시하는 별도의 `useEffect` 훅이 있습니다. `isCameraOn`이 `true`로 바뀌면, 이 훅이 `requestAnimationFrame(predictWebcam)`을 호출하여 예측 루프를 시작시킵니다. `isCameraOn`이 `false`로 바뀌면(`stopWebcam` 호출 시), `cancelAnimationFrame`을 호출하여 루프를 안전하게 중단합니다. **(React의 비동기적 상태 업데이트 문제를 해결한 핵심 로직)**
    4.  **실시간 예측 (`predictWebcam`):**
        *   `requestAnimationFrame`에 의해 계속해서 자신을 호출하며 루프를 형성합니다.
        *   현재 비디오 프레임과 `performance.now()`로 측정한 타임스탬프를 `poseLandmarkerRef.current.detectForVideo` 함수에 전달합니다.
        *   `detectForVideo` 함수의 세 번째 인자로 **콜백(Callback) 함수**를 제공합니다.
    5.  **결과 처리 및 그리기 (콜백 함수 내부):**
        *   AI 모델이 프레임 분석을 완료하면 이 콜백 함수가 비동기적으로 실행됩니다.
        *   콜백 함수는 먼저 캔버스를 깨끗이 지우고, 현재 비디오 화면을 그린 뒤, 그 위에 감지된 스켈레톤(랜드마크와 커넥터)을 그립니다. 이로써 비디오와 스켈레톤이 항상 동기화됩니다.
        *   감지된 랜드마크가 없으면 "강아지를 화면에 맞춰주세요"와 같은 피드백을 설정합니다.

### 4.3. 디버깅 과정 및 교훈

이번 기능 개발은 간단하지 않았으며, 다음과 같은 여러 심각한 문제들을 해결하는 과정이었습니다.

1.  **`resultListener` 방식의 실패:** 초기에는 `detectForVideo`의 콜백 대신 `createFromOptions`에 `resultListener`를 제공하는 방식을 시도했으나, 알 수 없는 이유로 콜백이 전혀 실행되지 않았습니다. **교훈:** 공식 예제에서 주로 사용하는, 그리고 더 직관적인 콜백 방식이 더 안정적일 수 있다.

2.  **라이브러리-WASM 버전 불일치:** `package.json`에 설치된 `@mediapipe/tasks-vision` 라이브러리 버전과 코드에서 CDN으로 불러오는 WASM 파일의 버전이 일치하지 않아 모델이 조용히 실패하는 문제가 있었습니다. **교훈:** MediaPipe와 같은 WASM 기반 라이브러리는 JS 코드와 WASM 엔진의 버전이 완벽하게 일치해야 한다.

3.  **`Date.now()` 타임스탬프 문제:** 로그 분석 결과, `Date.now()`가 MediaPipe가 요구하는 '단조롭게 증가하는(monotonically increasing)' 타임스탬프를 보장하지 않아 `Packet timestamp mismatch` 에러를 유발하는 것을 발견했습니다. **교훈:** `requestAnimationFrame`과 같은 고정밀 타이밍 작업에는 시스템 시간에 영향을 받는 `Date.now()` 대신, 항상 증가하는 것을 보장하는 `performance.now()`를 사용해야 한다.

4.  **React 상태 업데이트 시점 문제:** `setIsCameraOn(true)` 호출 직후에 루프 시작 함수를 호출했을 때, 아직 `isCameraOn` 값이 갱신되지 않아 루프가 시작되지 않는 문제가 있었습니다. **교훈:** React에서 상태 변경에 따른 후속 작업을 실행할 때는, 해당 상태를 의존성 배열(`dependency array`)에 넣은 `useEffect` 훅을 사용하는 것이 정석이다.